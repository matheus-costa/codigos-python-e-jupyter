{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ciD0SYJp2bUv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URL do arquivo de texto\n",
        "url = 'https://raw.githubusercontent.com/allanspadini/curso-tensorflow-proxima-palavra/main/dados/texto.txt'\n",
        "\n",
        "# Fazer o download do conteúdo do arquivo\n",
        "response = requests.get(url)\n",
        "texto = response.text\n",
        "\n",
        "# Dividir o conteúdo do arquivo em uma lista de strings, uma por linha\n",
        "corpus = texto.splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RKiVDUfb2hb1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 2000\n",
        "max_sequence_len = 50"
      ],
      "metadata": {
        "id": "2MLKmkzm2n-0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar o texto\n",
        "vectorizer = TextVectorization(max_tokens=max_vocab_size, output_sequence_length=max_sequence_len, output_mode='int')\n",
        "# Adaptar a camada ao corpus\n",
        "vectorizer.adapt(corpus)\n",
        "# Convertendo os textos em sequências de tokens\n",
        "tokenized_corpus = vectorizer(corpus)"
      ],
      "metadata": {
        "id": "nmGQB7YH5yhl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar as sequências de n-grams\n",
        "input_sequences = []\n",
        "for token_list in tokenized_corpus.numpy():\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "dAqhEqpY51QF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzGfo_Vo54X6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "42MXl3qC559o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences(sequences):\n",
        "    \"\"\"\n",
        "    Prepara as sequências para o modelo, removendo zeros à direita, adicionando padding à esquerda, truncado sequências longas e removendo sequências repetidas.\n",
        "\n",
        "    Args:\n",
        "        sequences: Um array de sequências (listas ou arrays NumPy).\n",
        "\n",
        "    Returns:\n",
        "        Um array NumPy 2D com as sequências preparadas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remover zeros à direita de cada sequência\n",
        "    sequences_without_trailing_zeros = []\n",
        "    for seq in sequences:\n",
        "        last_nonzero_index = np.argmax(seq[::-1] != 0)\n",
        "        if last_nonzero_index == 0 and seq[-1] == 0:\n",
        "            sequences_without_trailing_zeros.append(np.array([0]))\n",
        "        else:\n",
        "            sequences_without_trailing_zeros.append(seq[:-last_nonzero_index or None])\n",
        "\n",
        "    # Remover sequências repetidas\n",
        "    unique_sequences = []\n",
        "    for seq in sequences_without_trailing_zeros:\n",
        "        if seq.tolist() not in unique_sequences:  # Verifica se a sequência já está na lista\n",
        "            unique_sequences.append(seq.tolist())  # Adiciona à lista se for única\n",
        "\n",
        "    # Encontrar o comprimento máximo das sequências sem zeros à direita\n",
        "    max_sequence_len = max(len(seq) for seq in unique_sequences)\n",
        "\n",
        "    # Adicionar padding à esquerda para garantir o mesmo comprimento\n",
        "    padded_sequences = pad_sequences(unique_sequences, maxlen=max_sequence_len, padding='pre', truncating='post')\n",
        "\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "SGzTkZEa_pZI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences_prepared = prepare_sequences(input_sequences)\n",
        "print(input_sequences_prepared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlSGdIxnAPAq",
        "outputId": "db20ae6e-4dad-4d7d-aa37-9fd714d105c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0 449  46]\n",
            " [  0   0   0 ... 449  46 437]\n",
            " [  0   0   0 ...  46 437  12]\n",
            " ...\n",
            " [  0   0   0 ... 268   6  47]\n",
            " [  0   0   0 ...   6  47 213]\n",
            " [  0   0   0 ...  47 213 259]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar as features (entradas) e o label (saída)\n",
        "X = input_sequences_prepared[:,:-1]\n",
        "y = input_sequences_prepared[:,-1]"
      ],
      "metadata": {
        "id": "xGEmm78AARIh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(y, num_classes=max_vocab_size)"
      ],
      "metadata": {
        "id": "FybqYAmfATmX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o modelo\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=max_vocab_size,\n",
        "        output_dim=128,\n",
        "        mask_zero=False\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),  # Camada BiLSTM\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(max_vocab_size, activation='softmax')  # Use total_words em vez de 4 para o número de classes\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Treinar o modelo\n",
        "history = model.fit(X, y, epochs=20, verbose=1, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUHoz9nLAVuN",
        "outputId": "60556e5a-d36f-4dbc-ff6b-69c5974544e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "56/56 [==============================] - 12s 103ms/step - loss: 7.5560 - accuracy: 0.0099\n",
            "Epoch 2/20\n",
            "56/56 [==============================] - 3s 56ms/step - loss: 7.1972 - accuracy: 0.0438\n",
            "Epoch 3/20\n",
            "56/56 [==============================] - 2s 42ms/step - loss: 6.4090 - accuracy: 0.0828\n",
            "Epoch 4/20\n",
            "56/56 [==============================] - 2s 28ms/step - loss: 5.6172 - accuracy: 0.1252\n",
            "Epoch 5/20\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 5.1511 - accuracy: 0.1566\n",
            "Epoch 6/20\n",
            "56/56 [==============================] - 2s 37ms/step - loss: 4.8878 - accuracy: 0.1786\n",
            "Epoch 7/20\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 4.6832 - accuracy: 0.2029\n",
            "Epoch 8/20\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.4962 - accuracy: 0.2340\n",
            "Epoch 9/20\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 4.3290 - accuracy: 0.2518\n",
            "Epoch 10/20\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 4.1384 - accuracy: 0.2719\n",
            "Epoch 11/20\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 3.9607 - accuracy: 0.3010\n",
            "Epoch 12/20\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 3.8184 - accuracy: 0.3174\n",
            "Epoch 13/20\n",
            "56/56 [==============================] - 1s 18ms/step - loss: 3.6488 - accuracy: 0.3287\n",
            "Epoch 14/20\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 3.5165 - accuracy: 0.3414\n",
            "Epoch 15/20\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 3.3689 - accuracy: 0.3615\n",
            "Epoch 16/20\n",
            "56/56 [==============================] - 1s 25ms/step - loss: 3.2248 - accuracy: 0.3850\n",
            "Epoch 17/20\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 3.0704 - accuracy: 0.3932\n",
            "Epoch 18/20\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 2.9606 - accuracy: 0.4112\n",
            "Epoch 19/20\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 2.8456 - accuracy: 0.4257\n",
            "Epoch 20/20\n",
            "56/56 [==============================] - 1s 17ms/step - loss: 2.7254 - accuracy: 0.4361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(model, vectorizer, text, max_sequence_len, top_k=3):\n",
        "    \"\"\"\n",
        "    Prediz as próximas palavras mais prováveis em uma sequência de texto.\n",
        "\n",
        "    Args:\n",
        "        model: O modelo treinado.\n",
        "        vectorizer: A camada de vetorização.\n",
        "        text: O texto de entrada.\n",
        "        max_sequence_len: O comprimento máximo da sequência usado na vetorização.\n",
        "        top_k: O número de palavras mais prováveis a serem retornadas.\n",
        "\n",
        "    Returns:\n",
        "        As próximas palavras mais prováveis.\n",
        "    \"\"\"\n",
        "    # Vetorizar o texto de entrada\n",
        "    tokenized_text = vectorizer([text])\n",
        "\n",
        "    # Remover a dimensão extra adicionada pela vetorização\n",
        "    tokenized_text = np.squeeze(tokenized_text)\n",
        "\n",
        "    # Adicionar padding à esquerda\n",
        "    padded_text = pad_sequences([tokenized_text], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "    # Fazer a previsão\n",
        "    predicted_probs = model.predict(padded_text, verbose=0)[0]  # Remove a dimensão extra adicionada pela previsão\n",
        "\n",
        "    # Obter os índices dos top_k tokens com as maiores probabilidades\n",
        "    top_k_indices = np.argsort(predicted_probs)[-top_k:][::-1]\n",
        "\n",
        "    # Converter os tokens previstos de volta para palavras\n",
        "    predicted_words = [vectorizer.get_vocabulary()[index] for index in top_k_indices]\n",
        "\n",
        "    return predicted_words"
      ],
      "metadata": {
        "id": "zc9MGa1sCsb-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Transforme sua vida hoje! Com nosso novo produto, você não apenas alcançará seus objetivos, mas superará suas próprias expectativas. Invista em você    \""
      ],
      "metadata": {
        "id": "eT1_XpK-C9B8"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next_words(model, vectorizer, text, 50, top_k=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaQAvBH0DH8Z",
        "outputId": "cc3569be-26ec-46da-c340-175ef67716ca"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['especialistas',\n",
              " 'inteligente',\n",
              " 'mais',\n",
              " 'por',\n",
              " 'tempo',\n",
              " 'tudo',\n",
              " 'sobre',\n",
              " 'exclusivos',\n",
              " 'perder',\n",
              " 'se']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['especialistas',\n",
        " 'por',\n",
        " 'mais',\n",
        " 'inteligente',\n",
        " 'tudo',\n",
        " 'tempo',\n",
        " 'sobre',\n",
        " 'o',\n",
        " 'receitas',\n",
        " 'perder']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiCu0v8iDbcb",
        "outputId": "0294f1ff-653e-4554-8fca-a5130d1b6afa"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['especialistas',\n",
              " 'por',\n",
              " 'mais',\n",
              " 'inteligente',\n",
              " 'tudo',\n",
              " 'tempo',\n",
              " 'sobre',\n",
              " 'o',\n",
              " 'receitas',\n",
              " 'perder']"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "37WjLQ9xHZ6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}